import { create } from 'zustand'
import { devtools, persist } from 'zustand/middleware'

// AIæ¨¡å‹æ¥å£
interface AIModel {
  id: string
  name: string
  modelName: string
  displayName: string
  description?: string
  size?: string
  parameters?: string
  isAvailable?: boolean
}

// AIèŠå¤©æ¶ˆæ¯æ¥å£
interface ChatMessage {
  id: string
  role: 'user' | 'assistant' | 'system'
  content: string
  timestamp: Date
  isStreaming?: boolean
  thinkContent?: string
  executionTime?: number
}

// AIèŠå¤©ä¼šè¯æ¥å£
interface ChatSession {
  id: string
  title: string
  messages: ChatMessage[]
  createdAt: Date
  updatedAt: Date
}

// AI StoreçŠ¶æ€æ¥å£
interface AIState {
  // è¿æ¥çŠ¶æ€
  isConnected: boolean
  isConnecting: boolean
  connectionError: string | null
  ollamaConnected: boolean
  
  // Ollamaé…ç½®
  ollamaHost: string
  ollamaPort: number
  
  // æ¨¡å‹ç®¡ç†
  availableModels: AIModel[]
  currentModel: AIModel | null
  isLoadingModels: boolean
  isModelLoading: boolean
  
  // èŠå¤©åŠŸèƒ½
  chatSessions: ChatSession[]
  currentSessionId: string | null
  currentSession: ChatSession | null
  sessions: ChatSession[]
  isGenerating: boolean
  
  // æ–°å¢AIåŠŸèƒ½çŠ¶æ€
  smartQueryResult: {
    query: string
    explanation: string
    suggestions: string[]
  } | null
  performanceAnalysis: {
    optimizations: string[]
    indexSuggestions: string[]
    report: string
  } | null
  errorDiagnosis: {
    diagnosis: string
    solutions: string[]
    prevention: string[]
  } | null
}

// AI StoreåŠ¨ä½œæ¥å£
interface AIActions {
  // è¿æ¥ç®¡ç†
  connect: () => Promise<void>
  connectToOllama: (host: string, port: number) => Promise<void>
  disconnect: () => void
  setOllamaConfig: (host: string, port: number) => void
  
  // æ¨¡å‹ç®¡ç†
  loadModels: () => Promise<void>
  fetchAvailableModels: () => Promise<void>
  selectModel: (model: AIModel) => void
  setCurrentModel: (model: AIModel) => void
  
  // èŠå¤©åŠŸèƒ½
  createSession: (title?: string) => string
  selectSession: (sessionId: string) => void
  setCurrentSession: (sessionId: string) => void
  deleteSession: (sessionId: string) => void
  sendMessage: (content: string, sessionId?: string) => Promise<void>
  streamMessage: (content: string, sessionId?: string, onUpdate?: (content: string) => void) => Promise<void>
  clearSessions: () => void
  clearMessages: (sessionId: string) => void
  
  // æ–°å¢AIåŠŸèƒ½
  buildSmartQuery: (naturalLanguage: string, indexContext?: any) => Promise<{
    query: string
    explanation: string
    suggestions: string[]
  }>
  buildSmartQueryStream: (naturalLanguage: string, indexContext?: any, onUpdate?: (content: string) => void) => Promise<{
    query: string
    explanation: string
    suggestions: string[]
  }>
  analyzeQueryPerformance: (queryBody: any, queryResults: any) => Promise<{
    optimizations: string[]
    indexSuggestions: string[]
    report: string
  }>
  diagnoseError: (error: any, queryContext?: any) => Promise<{
    diagnosis: string
    solutions: string[]
    prevention: string[]
  }>
  getBestPractices: (queryType: string, dataCharacteristics?: any) => Promise<{
    practices: string[]
    examples: string[]
    warnings: string[]
  }>
}

type AIStore = AIState & AIActions

/**
 * åˆ›å»ºAI Store
 * ä½¿ç”¨persistä¸­é—´ä»¶è¿›è¡Œæ•°æ®æŒä¹…åŒ–ï¼Œç¡®ä¿èŠå¤©å†å²è®°å½•åœ¨é¡µé¢åˆ·æ–°åä¸ä¸¢å¤±
 */
export const useAIStore = create<AIStore>()(devtools(
  persist(
    (set, get) => {
      // è¾…åŠ©å‡½æ•°ï¼šæ›´æ–°è®¡ç®—å±æ€§
      const updateComputedProperties = () => {
        const state = get()
        const currentSession = state.chatSessions.find(s => s.id === state.currentSessionId) || null
        const sessions = state.chatSessions
        set({ currentSession, sessions })
      }

      // åˆå§‹åŒ–æ—¶æ›´æ–°è®¡ç®—å±æ€§
      setTimeout(() => updateComputedProperties(), 0)

      return {
        // åˆå§‹çŠ¶æ€
        isConnected: false,
        isConnecting: false,
        connectionError: null,
        ollamaConnected: false,
        ollamaHost: 'localhost',
        ollamaPort: 11434,
        availableModels: [],
        currentModel: null,
        isLoadingModels: false,
        isModelLoading: false,
        chatSessions: [],
        currentSessionId: null,
        currentSession: null as ChatSession | null,
        sessions: [] as ChatSession[],
        isGenerating: false,
        smartQueryResult: null,
        performanceAnalysis: null,
        errorDiagnosis: null,

        // è¿æ¥ç®¡ç†
        connect: async () => {
          set({ isConnecting: true, connectionError: null })
          
          try {
            const { ollamaHost, ollamaPort } = get()
            const response = await fetch(`http://${ollamaHost}:${ollamaPort}/api/tags`)
            
            if (!response.ok) {
              throw new Error(`HTTP ${response.status}: ${response.statusText}`)
            }
            
            set({ isConnected: true, isConnecting: false, ollamaConnected: true })
            
            // è‡ªåŠ¨åŠ è½½æ¨¡å‹åˆ—è¡¨
            await get().loadModels()
            
          } catch (error) {
            set({ 
              isConnected: false, 
              isConnecting: false,
              ollamaConnected: false,
              connectionError: error instanceof Error ? error.message : 'è¿æ¥å¤±è´¥'
            })
            throw error
          }
        },

        connectToOllama: async (host: string, port: number) => {
          set({ ollamaHost: host, ollamaPort: port })
          await get().connect()
        },

        disconnect: () => {
          set({ 
            isConnected: false, 
            ollamaConnected: false,
            connectionError: null,
            availableModels: [],
            currentModel: null
          })
        },

        setOllamaConfig: (host: string, port: number) => {
          set({ ollamaHost: host, ollamaPort: port })
        },

        // æ¨¡å‹ç®¡ç†
        loadModels: async () => {
          const { ollamaHost, ollamaPort } = get()
          set({ isLoadingModels: true, isModelLoading: true })
          
          try {
            const response = await fetch(`http://${ollamaHost}:${ollamaPort}/api/tags`)
            
            if (!response.ok) {
              throw new Error(`HTTP ${response.status}: ${response.statusText}`)
            }
            
            const data = await response.json()
            const models: AIModel[] = data.models?.map((model: any) => ({
              id: model.name,
              name: model.name,
              modelName: model.name,
              displayName: model.name,
              description: model.details?.family || '',
              size: model.size ? `${(model.size / 1024 / 1024 / 1024).toFixed(1)}GB` : '',
              parameters: model.details?.parameter_size || '',
              isAvailable: true
            })) || []
            
            set({ availableModels: models, isLoadingModels: false, isModelLoading: false })
            
            // å¦‚æœæ²¡æœ‰é€‰æ‹©æ¨¡å‹ä¸”æœ‰å¯ç”¨æ¨¡å‹ï¼Œè‡ªåŠ¨é€‰æ‹©ç¬¬ä¸€ä¸ª
            if (!get().currentModel && models.length > 0) {
              set({ currentModel: models[0] })
            }
            
          } catch (error) {
            set({ isLoadingModels: false, isModelLoading: false })
            throw error
          }
        },

        fetchAvailableModels: async () => {
          await get().loadModels()
        },

        selectModel: (model: AIModel) => {
          set({ currentModel: model })
        },

        setCurrentModel: (model: AIModel) => {
          set({ currentModel: model })
        },

        // èŠå¤©åŠŸèƒ½
        createSession: (title?: string) => {
          const sessionId = `session_${Date.now()}`
          const newSession: ChatSession = {
            id: sessionId,
            title: title || `å¯¹è¯ ${new Date().toLocaleString()}`,
            messages: [],
            createdAt: new Date(),
            updatedAt: new Date()
          }
          
          set(state => ({
            chatSessions: [newSession, ...state.chatSessions],
            currentSessionId: sessionId
          }))
          
          updateComputedProperties()
          return sessionId
        },

        selectSession: (sessionId: string) => {
          set({ currentSessionId: sessionId })
          updateComputedProperties()
        },

        setCurrentSession: (sessionId: string) => {
          set({ currentSessionId: sessionId })
          updateComputedProperties()
        },

        deleteSession: (sessionId: string) => {
          set(state => {
            const newSessions = state.chatSessions.filter(s => s.id !== sessionId)
            const newCurrentSessionId = state.currentSessionId === sessionId 
              ? (newSessions.length > 0 ? newSessions[0].id : null)
              : state.currentSessionId
            
            return {
              chatSessions: newSessions,
              currentSessionId: newCurrentSessionId
            }
          })
          updateComputedProperties()
        },

        sendMessage: async (content: string, sessionId?: string) => {
          const { currentSessionId, currentModel, ollamaHost, ollamaPort } = get()
          const targetSessionId = sessionId || currentSessionId
          
          if (!targetSessionId) {
            throw new Error('æ²¡æœ‰æ´»åŠ¨çš„èŠå¤©ä¼šè¯')
          }
          
          if (!currentModel) {
            throw new Error('æœªé€‰æ‹©AIæ¨¡å‹')
          }
          
          // æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
          const userMessage: ChatMessage = {
            id: `msg_${Date.now()}_user`,
            role: 'user',
            content,
            timestamp: new Date()
          }
          
          set(state => ({
            chatSessions: state.chatSessions.map(session => 
              session.id === targetSessionId
                ? { ...session, messages: [...session.messages, userMessage], updatedAt: new Date() }
                : session
            ),
            isGenerating: true
          }))
          updateComputedProperties()
          
          try {
            // è·å–ä¼šè¯å†å²
            const session = get().chatSessions.find(s => s.id === targetSessionId)
            const messages = session?.messages || []
            
            // æ„å»ºå¯¹è¯ä¸Šä¸‹æ–‡
            const prompt = messages.map(msg => 
              `${msg.role === 'user' ? 'Human' : 'Assistant'}: ${msg.content}`
            ).join('\n\n') + '\n\nAssistant: '
            
            const response = await fetch(`http://${ollamaHost}:${ollamaPort}/api/generate`, {
              method: 'POST',
              headers: { 'Content-Type': 'application/json' },
              body: JSON.stringify({
                model: currentModel.modelName,
                prompt,
                stream: false
              })
            })
            
            if (!response.ok) {
              throw new Error(`HTTP ${response.status}: ${response.statusText}`)
            }
            
            const data = await response.json()
            
            // æ·»åŠ AIå›å¤
            const assistantMessage: ChatMessage = {
              id: `msg_${Date.now()}_assistant`,
              role: 'assistant',
              content: data.response || 'æŠ±æ­‰ï¼Œæˆ‘æ— æ³•ç”Ÿæˆå›å¤ã€‚',
              timestamp: new Date()
            }
            
            set(state => ({
              chatSessions: state.chatSessions.map(session => 
                session.id === targetSessionId
                  ? { ...session, messages: [...session.messages, assistantMessage], updatedAt: new Date() }
                  : session
              ),
              isGenerating: false
            }))
            updateComputedProperties()
            
          } catch (error) {
            set({ isGenerating: false })
            throw error
          }
        },

        streamMessage: async (content: string, sessionId?: string, onUpdate?: (content: string) => void) => {
          const { currentSessionId, currentModel, ollamaHost, ollamaPort } = get()
          const targetSessionId = sessionId || currentSessionId
          
          if (!targetSessionId) {
            throw new Error('æ²¡æœ‰æ´»åŠ¨çš„èŠå¤©ä¼šè¯')
          }
          
          if (!currentModel) {
            throw new Error('æœªé€‰æ‹©AIæ¨¡å‹')
          }
          
          // æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
          const userMessage: ChatMessage = {
            id: `msg_${Date.now()}_user`,
            role: 'user',
            content,
            timestamp: new Date()
          }
          
          // åˆ›å»ºæµå¼AIæ¶ˆæ¯
          const assistantMessageId = `msg_${Date.now()}_assistant`
          const assistantMessage: ChatMessage = {
              id: assistantMessageId,
              role: 'assistant',
              content: '',
              timestamp: new Date(),
              isStreaming: true
            }
          
          set(state => {
            const newState = {
              chatSessions: state.chatSessions.map(session => 
                session.id === targetSessionId
                  ? { 
                      ...session, 
                      messages: [...session.messages, userMessage, assistantMessage], 
                      updatedAt: new Date() 
                    }
                  : session
              ),
              isGenerating: true
            }
            updateComputedProperties()
            return newState
          })
          
          try {
            // è·å–ä¼šè¯å†å²
            const session = get().chatSessions.find(s => s.id === targetSessionId)
            const messages = session?.messages.slice(0, -1) || [] // æ’é™¤åˆšæ·»åŠ çš„ç©ºåŠ©æ‰‹æ¶ˆæ¯
            
            // æ„å»ºå¯¹è¯ä¸Šä¸‹æ–‡
            const prompt = messages.map(msg => 
              `${msg.role === 'user' ? 'Human' : 'Assistant'}: ${msg.content}`
            ).join('\n\n') + '\n\nAssistant: '
            
            const response = await fetch(`http://${ollamaHost}:${ollamaPort}/api/generate`, {
              method: 'POST',
              headers: { 'Content-Type': 'application/json' },
              body: JSON.stringify({
                model: currentModel.modelName,
                prompt,
                stream: true
              })
            })
            
            if (!response.ok) {
              throw new Error(`HTTP ${response.status}: ${response.statusText}`)
            }
            
            const reader = response.body?.getReader()
            const decoder = new TextDecoder()
            let fullContent = ''
            
            if (reader) {
              while (true) {
                const { done, value } = await reader.read()
                if (done) break
                
                const chunk = decoder.decode(value)
                const lines = chunk.split('\n').filter(line => line.trim())
                
                for (const line of lines) {
                  try {
                    const data = JSON.parse(line)
                    if (data.response) {
                      fullContent += data.response
                      
                      // æ›´æ–°æ¶ˆæ¯å†…å®¹
                      set(state => {
                        const newState = {
                          chatSessions: state.chatSessions.map(session => 
                            session.id === targetSessionId
                              ? {
                                  ...session,
                                  messages: session.messages.map(msg => 
                                    msg.id === assistantMessageId
                                      ? { ...msg, content: fullContent }
                                      : msg
                                  ),
                                  updatedAt: new Date()
                                }
                              : session
                          )
                        }
                        // ç¡®ä¿è®¡ç®—å±æ€§æ›´æ–°
                        updateComputedProperties()
                        return newState
                      })
                      
                      // è°ƒç”¨æ›´æ–°å›è°ƒ
                      onUpdate?.(fullContent)
                    }
                  } catch {
                    // å¿½ç•¥è§£æé”™è¯¯çš„è¡Œ
                  }
                }
              }
            }
            
            // å®Œæˆæµå¼ä¼ è¾“
            set(state => {
              const newState = {
                chatSessions: state.chatSessions.map(session => 
                  session.id === targetSessionId
                    ? {
                        ...session,
                        messages: session.messages.map(msg => 
                          msg.id === assistantMessageId
                            ? { ...msg, isStreaming: false }
                            : msg
                        ),
                        updatedAt: new Date()
                      }
                    : session
                ),
                isGenerating: false
              }
              // ç¡®ä¿è®¡ç®—å±æ€§æ›´æ–°
              updateComputedProperties()
              return newState
            })
            
          } catch (error) {
            set({ isGenerating: false })
            throw error
          }
        },

        clearSessions: () => {
          set({ 
            chatSessions: [], 
            currentSessionId: null,
            currentSession: null,
            sessions: []
          })
        },

        clearMessages: (sessionId: string) => {
          set(state => ({
            chatSessions: state.chatSessions.map(session => 
              session.id === sessionId
                ? { ...session, messages: [], updatedAt: new Date() }
                : session
            )
          }))
          updateComputedProperties()
        },

        // æ–°å¢AIåŠŸèƒ½å®ç°
        /**
         * æ™ºèƒ½æŸ¥è¯¢æ„å»ºå™¨
         * æ ¹æ®è‡ªç„¶è¯­è¨€å’Œç´¢å¼•ä¸Šä¸‹æ–‡ç”Ÿæˆç²¾ç¡®çš„æŸ¥è¯¢
         */
        buildSmartQuery: async (naturalLanguage, indexContext) => {
          set({ isGenerating: true })

          try {
            const { currentModel, ollamaHost, ollamaPort } = get()
            
            if (!currentModel) {
              throw new Error('æœªé€‰æ‹©AIæ¨¡å‹')
            }

            const prompt = `ä½ æ˜¯ä¸€ä¸ªElasticsearchæ™ºèƒ½æŸ¥è¯¢æ„å»ºä¸“å®¶ã€‚è¯·æ ¹æ®ç”¨æˆ·çš„è‡ªç„¶è¯­è¨€æè¿°å’Œç´¢å¼•ç»“æ„ä¿¡æ¯ï¼Œç”Ÿæˆæœ€ç²¾ç¡®çš„ElasticsearchæŸ¥è¯¢ã€‚

ç”¨æˆ·æè¿°: ${naturalLanguage}

${indexContext ? `ç´¢å¼•ä¸Šä¸‹æ–‡ä¿¡æ¯:
- ç´¢å¼•åç§°: ${indexContext.indexName || 'æœªçŸ¥'}
- å­—æ®µæ˜ å°„: ${JSON.stringify(indexContext.mapping || {}, null, 2)}
- ç´¢å¼•è®¾ç½®: ${JSON.stringify(indexContext.settings || {}, null, 2)}
- æ–‡æ¡£æ ·ä¾‹: ${JSON.stringify(indexContext.sampleDoc || {}, null, 2)}` : ''}

è¯·è¿”å›JSONæ ¼å¼çš„å“åº”ï¼ŒåŒ…å«:
{
  "query": "ç”Ÿæˆçš„DSLæŸ¥è¯¢(JSONå­—ç¬¦ä¸²)",
  "explanation": "æŸ¥è¯¢é€»è¾‘çš„è¯¦ç»†è§£é‡Š",
  "suggestions": ["ä¼˜åŒ–å»ºè®®1", "ä¼˜åŒ–å»ºè®®2"]
}`

            const response = await fetch(`http://${ollamaHost}:${ollamaPort}/api/generate`, {
              method: 'POST',
              headers: { 'Content-Type': 'application/json' },
              body: JSON.stringify({
                model: currentModel.modelName,
                prompt,
                stream: false,
                options: {
                  temperature: 0.3,
                  num_predict: 2048,
                }
              })
            })

            if (!response.ok) {
              throw new Error(`HTTP ${response.status}: ${response.statusText}`)
            }

            const data = await response.json()
            
            try {
              const result = JSON.parse(data.response)
              return {
                query: result.query || '{}',
                explanation: result.explanation || 'æŸ¥è¯¢ç”Ÿæˆå®Œæˆ',
                suggestions: result.suggestions || []
              }
            } catch {
              // å¦‚æœAIè¿”å›çš„ä¸æ˜¯JSONæ ¼å¼ï¼Œå°è¯•è§£æ
              return {
                query: '{}',
                explanation: data.response || 'æŸ¥è¯¢ç”Ÿæˆå¤±è´¥',
                suggestions: ['è¯·æ£€æŸ¥è¾“å…¥çš„è‡ªç„¶è¯­è¨€æè¿°']
              }
            }

          } catch (error) {
            throw new Error('æ™ºèƒ½æŸ¥è¯¢æ„å»ºå¤±è´¥: ' + (error instanceof Error ? error.message : 'æœªçŸ¥é”™è¯¯'))
          } finally {
            set({ isGenerating: false })
          }
        },

        /**
         * æµå¼æ™ºèƒ½æŸ¥è¯¢æ„å»ºå™¨
         * æ ¹æ®è‡ªç„¶è¯­è¨€å’Œç´¢å¼•ä¸Šä¸‹æ–‡ç”Ÿæˆç²¾ç¡®çš„æŸ¥è¯¢ï¼Œæ”¯æŒæµå¼è¾“å‡º
         */
        buildSmartQueryStream: async (naturalLanguage, indexContext, onUpdate) => {
          set({ isGenerating: true })

          try {
            const { currentModel, ollamaHost, ollamaPort } = get()
            
            if (!currentModel) {
              throw new Error('æœªé€‰æ‹©AIæ¨¡å‹')
            }

            const prompt = `ä½ æ˜¯ä¸€ä¸ªElasticsearchæ™ºèƒ½æŸ¥è¯¢æ„å»ºä¸“å®¶ã€‚è¯·æ ¹æ®ç”¨æˆ·çš„è‡ªç„¶è¯­è¨€æè¿°å’Œç´¢å¼•ç»“æ„ä¿¡æ¯ï¼Œç”Ÿæˆæœ€ç²¾ç¡®çš„ElasticsearchæŸ¥è¯¢ã€‚

ç”¨æˆ·æè¿°: ${naturalLanguage}

${indexContext ? `ç´¢å¼•ä¸Šä¸‹æ–‡ä¿¡æ¯:
- ç´¢å¼•åç§°: ${indexContext.indexName || 'æœªçŸ¥'}
- å­—æ®µæ˜ å°„: ${JSON.stringify(indexContext.mapping || {}, null, 2)}
- ç´¢å¼•è®¾ç½®: ${JSON.stringify(indexContext.settings || {}, null, 2)}
- æ–‡æ¡£æ ·ä¾‹: ${JSON.stringify(indexContext.sampleDoc || {}, null, 2)}` : ''}

è¯·è¿”å›JSONæ ¼å¼çš„å“åº”ï¼ŒåŒ…å«:
{
  "query": "ç”Ÿæˆçš„DSLæŸ¥è¯¢(JSONå­—ç¬¦ä¸²)",
  "explanation": "æŸ¥è¯¢é€»è¾‘çš„è¯¦ç»†è§£é‡Š",
  "suggestions": ["ä¼˜åŒ–å»ºè®®1", "ä¼˜åŒ–å»ºè®®2"]
}`

            const response = await fetch(`http://${ollamaHost}:${ollamaPort}/api/generate`, {
              method: 'POST',
              headers: { 'Content-Type': 'application/json' },
              body: JSON.stringify({
                model: currentModel.modelName,
                prompt,
                stream: true,
                options: {
                  temperature: 0.3,
                  num_predict: 2048,
                }
              })
            })

            if (!response.ok) {
              throw new Error(`HTTP ${response.status}: ${response.statusText}`)
            }

            const reader = response.body?.getReader()
            const decoder = new TextDecoder()
            let fullResponse = ''

            if (reader) {
              while (true) {
                const { done, value } = await reader.read()
                if (done) break

                const chunk = decoder.decode(value)
                const lines = chunk.split('\n').filter(line => line.trim())
                
                for (const line of lines) {
                  try {
                    const data = JSON.parse(line)
                    if (data.response) {
                      fullResponse += data.response
                      
                      // ä¸ºæµå¼è¾“å‡ºæä¾›æ›´å¥½çš„æ˜¾ç¤ºå†…å®¹
                      let displayContent = 'ğŸ¤– AIæ­£åœ¨åˆ†ææ‚¨çš„æŸ¥è¯¢éœ€æ±‚...\n\n'
                      
                      // å°è¯•æ£€æµ‹æ˜¯å¦å¼€å§‹ç”ŸæˆJSON
                      if (fullResponse.includes('{')) {
                        displayContent += 'ğŸ“ æ­£åœ¨æ„å»ºElasticsearchæŸ¥è¯¢...\n\n'
                        
                        // å¦‚æœåŒ…å«queryå­—æ®µï¼Œæ˜¾ç¤ºæ­£åœ¨ç”ŸæˆæŸ¥è¯¢
                        if (fullResponse.includes('"query"')) {
                          displayContent += 'ğŸ” æ­£åœ¨ç”ŸæˆæŸ¥è¯¢ç»“æ„...\n\n'
                        }
                        
                        // å¦‚æœåŒ…å«explanationå­—æ®µï¼Œæ˜¾ç¤ºæ­£åœ¨ç”Ÿæˆè¯´æ˜
                        if (fullResponse.includes('"explanation"')) {
                          displayContent += 'ğŸ“– æ­£åœ¨ç”ŸæˆæŸ¥è¯¢è¯´æ˜...\n\n'
                        }
                        
                        // å¦‚æœåŒ…å«suggestionså­—æ®µï¼Œæ˜¾ç¤ºæ­£åœ¨ç”Ÿæˆå»ºè®®
                        if (fullResponse.includes('"suggestions"')) {
                          displayContent += 'ğŸ’¡ æ­£åœ¨ç”Ÿæˆä¼˜åŒ–å»ºè®®...\n\n'
                        }
                      }
                      
                      displayContent += `åŸå§‹å“åº”é•¿åº¦: ${fullResponse.length} å­—ç¬¦`
                      
                      // æ·»åŠ è°ƒè¯•ä¿¡æ¯
                      console.log('æµå¼æ›´æ–°:', displayContent.substring(0, 100) + '...')
                      onUpdate?.(displayContent)
                    }
                  } catch {
                    // å¿½ç•¥è§£æé”™è¯¯çš„è¡Œ
                  }
                }
              }
            }

            try {
              const result = JSON.parse(fullResponse)
              return {
                query: result.query || '{}',
                explanation: result.explanation || 'æŸ¥è¯¢ç”Ÿæˆå®Œæˆ',
                suggestions: result.suggestions || []
              }
            } catch {
              // å¦‚æœAIè¿”å›çš„ä¸æ˜¯JSONæ ¼å¼ï¼Œå°è¯•è§£æ
              return {
                query: '{}',
                explanation: fullResponse || 'æŸ¥è¯¢ç”Ÿæˆå¤±è´¥',
                suggestions: ['è¯·æ£€æŸ¥è¾“å…¥çš„è‡ªç„¶è¯­è¨€æè¿°']
              }
            }

          } catch (error) {
            throw new Error('æ™ºèƒ½æŸ¥è¯¢æ„å»ºå¤±è´¥: ' + (error instanceof Error ? error.message : 'æœªçŸ¥é”™è¯¯'))
          } finally {
            set({ isGenerating: false })
          }
        },

        /**
         * æŸ¥è¯¢æ€§èƒ½åˆ†æ
         * åˆ†ææŸ¥è¯¢æ‰§è¡Œç»“æœï¼Œæä¾›æ€§èƒ½ä¼˜åŒ–å»ºè®®
         */
        analyzeQueryPerformance: async (queryBody, queryResults) => {
          set({ isGenerating: true })

          try {
            const { currentModel, ollamaHost, ollamaPort } = get()
            
            if (!currentModel) {
              throw new Error('æœªé€‰æ‹©AIæ¨¡å‹')
            }

            const prompt = `ä½ æ˜¯ä¸€ä¸ªElasticsearchæ€§èƒ½ä¼˜åŒ–ä¸“å®¶ã€‚è¯·åˆ†æä»¥ä¸‹æŸ¥è¯¢å’Œæ‰§è¡Œç»“æœï¼Œæä¾›è¯¦ç»†çš„æ€§èƒ½ä¼˜åŒ–å»ºè®®ã€‚

æŸ¥è¯¢å†…å®¹:
${JSON.stringify(queryBody, null, 2)}

æ‰§è¡Œç»“æœ:
- æ€»å‘½ä¸­æ•°: ${queryResults.hits?.total?.value || 0}
- æ‰§è¡Œæ—¶é—´: ${queryResults.took || 0}ms
- åˆ†ç‰‡ä¿¡æ¯: ${JSON.stringify(queryResults._shards || {}, null, 2)}

è¯·è¿”å›JSONæ ¼å¼çš„å“åº”ï¼ŒåŒ…å«:
{
  "optimizations": ["ä¼˜åŒ–å»ºè®®1", "ä¼˜åŒ–å»ºè®®2"],
  "indexSuggestions": ["ç´¢å¼•å»ºè®®1", "ç´¢å¼•å»ºè®®2"],
  "report": "è¯¦ç»†çš„æ€§èƒ½åˆ†ææŠ¥å‘Š"
}`

            const response = await fetch(`http://${ollamaHost}:${ollamaPort}/api/generate`, {
              method: 'POST',
              headers: { 'Content-Type': 'application/json' },
              body: JSON.stringify({
                model: currentModel.modelName,
                prompt,
                stream: false,
                options: {
                  temperature: 0.3,
                  num_predict: 2048,
                }
              })
            })

            if (!response.ok) {
              throw new Error(`HTTP ${response.status}: ${response.statusText}`)
            }

            const data = await response.json()
            
            try {
              const result = JSON.parse(data.response)
              return {
                optimizations: result.optimizations || [],
                indexSuggestions: result.indexSuggestions || [],
                report: result.report || 'æ€§èƒ½åˆ†æå®Œæˆ'
              }
            } catch {
              return {
                optimizations: ['è¯·æ£€æŸ¥æŸ¥è¯¢ç»“æ„'],
                indexSuggestions: ['è€ƒè™‘æ·»åŠ ç›¸å…³ç´¢å¼•'],
                report: data.response || 'æ€§èƒ½åˆ†æå¤±è´¥'
              }
            }

          } catch (error) {
            throw new Error('æ€§èƒ½åˆ†æå¤±è´¥: ' + (error instanceof Error ? error.message : 'æœªçŸ¥é”™è¯¯'))
          } finally {
            set({ isGenerating: false })
          }
        },

        /**
         * æŸ¥è¯¢é”™è¯¯è¯Šæ–­
         * æä¾›è¯¦ç»†çš„é”™è¯¯è¯Šæ–­å’Œè§£å†³æ–¹æ¡ˆ
         */
        diagnoseError: async (error, queryContext) => {
          set({ isGenerating: true })

          try {
            const { currentModel, ollamaHost, ollamaPort } = get()
            
            if (!currentModel) {
              throw new Error('æœªé€‰æ‹©AIæ¨¡å‹')
            }

            const prompt = `ä½ æ˜¯ä¸€ä¸ªElasticsearché”™è¯¯è¯Šæ–­ä¸“å®¶ã€‚è¯·åˆ†æä»¥ä¸‹é”™è¯¯ä¿¡æ¯å’ŒæŸ¥è¯¢ä¸Šä¸‹æ–‡ï¼Œæä¾›è¯¦ç»†çš„è¯Šæ–­å’Œè§£å†³æ–¹æ¡ˆã€‚

é”™è¯¯ä¿¡æ¯:
${JSON.stringify(error, null, 2)}

æŸ¥è¯¢ä¸Šä¸‹æ–‡:
${JSON.stringify(queryContext || {}, null, 2)}

è¯·è¿”å›JSONæ ¼å¼çš„å“åº”ï¼ŒåŒ…å«:
{
  "diagnosis": "é”™è¯¯åŸå› åˆ†æ",
  "solutions": ["è§£å†³æ–¹æ¡ˆ1", "è§£å†³æ–¹æ¡ˆ2"],
  "prevention": ["é¢„é˜²æªæ–½1", "é¢„é˜²æªæ–½2"]
}`

            const response = await fetch(`http://${ollamaHost}:${ollamaPort}/api/generate`, {
              method: 'POST',
              headers: { 'Content-Type': 'application/json' },
              body: JSON.stringify({
                model: currentModel.modelName,
                prompt,
                stream: false,
                options: {
                  temperature: 0.3,
                  num_predict: 2048,
                }
              })
            })

            if (!response.ok) {
              throw new Error(`HTTP ${response.status}: ${response.statusText}`)
            }

            const data = await response.json()
            
            try {
              const result = JSON.parse(data.response)
              return {
                diagnosis: result.diagnosis || 'é”™è¯¯è¯Šæ–­å®Œæˆ',
                solutions: result.solutions || [],
                prevention: result.prevention || []
              }
            } catch {
              return {
                diagnosis: data.response || 'é”™è¯¯è¯Šæ–­å¤±è´¥',
                solutions: ['è¯·æ£€æŸ¥æŸ¥è¯¢è¯­æ³•'],
                prevention: ['éµå¾ªæœ€ä½³å®è·µ']
              }
            }

          } catch (error) {
            throw new Error('é”™è¯¯è¯Šæ–­å¤±è´¥: ' + (error instanceof Error ? error.message : 'æœªçŸ¥é”™è¯¯'))
          } finally {
            set({ isGenerating: false })
          }
        },

        /**
         * æœ€ä½³å®è·µå»ºè®®
         * æ ¹æ®æŸ¥è¯¢ç±»å‹å’Œæ•°æ®ç‰¹å¾æä¾›æœ€ä½³å®è·µå»ºè®®
         */
        getBestPractices: async (queryType, dataCharacteristics) => {
          set({ isGenerating: true })

          try {
            const { currentModel, ollamaHost, ollamaPort } = get()
            
            if (!currentModel) {
              throw new Error('æœªé€‰æ‹©AIæ¨¡å‹')
            }

            const prompt = `ä½ æ˜¯ä¸€ä¸ªElasticsearchæœ€ä½³å®è·µä¸“å®¶ã€‚è¯·æ ¹æ®æŸ¥è¯¢ç±»å‹å’Œæ•°æ®ç‰¹å¾ï¼Œæä¾›ç›¸åº”çš„æœ€ä½³å®è·µå»ºè®®ã€‚

æŸ¥è¯¢ç±»å‹: ${queryType}
æ•°æ®ç‰¹å¾: ${JSON.stringify(dataCharacteristics || {}, null, 2)}

è¯·è¿”å›JSONæ ¼å¼çš„å“åº”ï¼ŒåŒ…å«:
{
  "practices": ["æœ€ä½³å®è·µ1", "æœ€ä½³å®è·µ2"],
  "examples": ["ç¤ºä¾‹1", "ç¤ºä¾‹2"],
  "warnings": ["æ³¨æ„äº‹é¡¹1", "æ³¨æ„äº‹é¡¹2"]
}`

            const response = await fetch(`http://${ollamaHost}:${ollamaPort}/api/generate`, {
              method: 'POST',
              headers: { 'Content-Type': 'application/json' },
              body: JSON.stringify({
                model: currentModel.modelName,
                prompt,
                stream: false,
                options: {
                  temperature: 0.3,
                  num_predict: 2048,
                }
              })
            })

            if (!response.ok) {
              throw new Error(`HTTP ${response.status}: ${response.statusText}`)
            }

            const data = await response.json()
            
            try {
              const result = JSON.parse(data.response)
              return {
                practices: result.practices || [],
                examples: result.examples || [],
                warnings: result.warnings || []
              }
            } catch {
              return {
                practices: ['éµå¾ªElasticsearchå®˜æ–¹æ–‡æ¡£'],
                examples: ['å‚è€ƒå®˜æ–¹ç¤ºä¾‹'],
                warnings: [data.response || 'æœ€ä½³å®è·µå»ºè®®ç”Ÿæˆå¤±è´¥']
              }
            }

          } catch (error) {
            throw new Error('æœ€ä½³å®è·µå»ºè®®ç”Ÿæˆå¤±è´¥: ' + (error instanceof Error ? error.message : 'æœªçŸ¥é”™è¯¯'))
          } finally {
            set({ isGenerating: false })
          }
        }
      }
    },
    {
      name: 'ai-store',
      // åªæŒä¹…åŒ–èŠå¤©ä¼šè¯ç›¸å…³æ•°æ®
      partialize: (state) => ({
        chatSessions: state.chatSessions,
        currentSessionId: state.currentSessionId,
        ollamaHost: state.ollamaHost,
        ollamaPort: state.ollamaPort,
        currentModel: state.currentModel
      })
    }
  ),
  {
    name: 'ai-store-devtools'
  }
))

export type { AIModel, ChatMessage, ChatSession, AIState, AIActions }